{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some matrix algebra\n",
    "\n",
    "- Scalars are numbers\n",
    "- Vectors are columns\n",
    "- Transposed vectors are rows\n",
    "- A matrix has rows and columns\n",
    "\n",
    "- Multiplying two matrices is only possible if dimension 2 of array 1 == dim1 of array 2\n",
    "- the resultant array shape is (array1dim1, array2dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 7 2 1 2 6 7 3 2 1]\n",
      " [4 5 2 8 2 1 4 2 6 7]]\n",
      "[7 5 8 1 6 6 1 7 3 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,2) and (10,) not aligned: 2 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-dcf8af028edc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mv0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,2) and (10,) not aligned: 2 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "v0 = np.random.randint(1,10,[2,10])\n",
    "v1 = np.random.randint(1,10,10)\n",
    "print(v0)\n",
    "print(v1)\n",
    "v0.T.dot(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few machine learning shorthands:\n",
    "\n",
    "- x: inputs\n",
    "- y: actual outputs\n",
    "- $\\hat{y}$: Predicted outputs\n",
    "- z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function:\n",
    "\n",
    "$\\hat{y} = \\sigma ({w^T}x + b) $\n",
    "\n",
    "where:\n",
    "\n",
    "## $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Intuitively, a higher z value (depicted on the x axis of a sigmoid chart) results in a $\\sigma (z)$ closer to 1, and vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for logistic regression:\n",
    "You want predicted $\\hat y$ to approximate actual $y$\n",
    "\n",
    "### Loss$(\\hat y, y) = -(ylog\\hat y + (1 - y)log(1 - \\hat y))$\n",
    "\n",
    "Intuitively, if $y = 0$, then the function shortens to: \n",
    "\n",
    "$-log(1- \\hat y)$,\n",
    "\n",
    "...which implies that a small $\\hat y$ leads to a large $log(1 - \\hat y)$, which leads to a lower \"cost\" (because of the -ve sign)\n",
    "\n",
    "\n",
    "... and if $y = 1$ then the formula shortens to:\n",
    "\n",
    "$- y log \\hat y$, $y^{aa}$\n",
    "\n",
    "... so a larger $\\hat y$ leads to a small $ylog\\hat y$, which leads to a small \"loss\" because of the -ve\n",
    "\n",
    "### and the Cost function is just an average of all the loss functions:\n",
    "#### the cost is referred to as $J$\n",
    "\n",
    "### $J(w,b) = \\frac{- \\displaystyle\\sum_{i=1}^{m}ylog\\hat y + (1 - y)log(1 - \\hat y)}{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "We want to find $w, b$ that minimize $J(w,b)$\n",
    "\n",
    "$ w:= w - \\alpha\\frac{dJ(w)}{dw} $,\n",
    "\n",
    "where:\n",
    "\n",
    "$w:=$ means repeatedly update $w$\n",
    "\n",
    "$\\alpha$ = learning rate\n",
    "\n",
    "$\\frac{dJ(w)}{dw}$: the change in the cost function with respect to the change in $w$. Note that this is sometimes referred to as just $dw$ in python variables.\n",
    "\n",
    "Note also that because our cost function is dependant on two variables ($w$ and $b$), we will actually be solving two \"partial\" derivatives, and some text books change the $d$ notiation to $\\delta$, i.e.:\n",
    "\n",
    "$ w:= w - \\alpha\\frac{\\delta J(w, b)}{\\delta w} $, and\n",
    "\n",
    "$ b:= b - \\alpha\\frac{\\delta J(w, b)}{\\delta b} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
